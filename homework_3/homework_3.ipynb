{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "homework.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WVASxK_7oqI"
      },
      "source": [
        "# Homework 3\n",
        "\n",
        "Hi! Today you have a difficult task.\n",
        "You need to implement two ideas to upgrade the metric learning pipeline: HardClusterSampler and HierarchicalTripletLoss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb5_yl-aVDxT"
      },
      "source": [
        "!pip install -U --quiet catalyst albumentations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn1ibXQbVHhN"
      },
      "source": [
        "from catalyst.utils import set_global_seed\n",
        "\n",
        "\n",
        "set_global_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcERM9BxVH1F"
      },
      "source": [
        "# Metric Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxWXtT02VJSV"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3Ag8n9LVNXY"
      },
      "source": [
        "## Omniglot\n",
        "\n",
        "Remember dataset: [Omniglot](https://github.com/brendenlake/omniglot). It contains 1623 different handwritten characters from 50 different alphabets. They have been written by 20 different people and collect via Amazon's Mechanical Turk. Some of the characters are presented on the picture:\n",
        "\n",
        "![photo](https://raw.githubusercontent.com/brendenlake/omniglot/master/omniglot_grid.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeSQX6O2VOJz"
      },
      "source": [
        "# Download dataset\n",
        "!git clone https://github.com/brendenlake/omniglot\n",
        "!unzip omniglot/python/images_background\n",
        "!unzip omniglot/python/images_evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ek5rO5bIJtb"
      },
      "source": [
        "Look at the file structure. Each folder is called after alphabetic system and contains character's folder with drawn examples.\n",
        "\n",
        "Let's show the `a` character from the Latin alphabet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpXIVViPqWTv"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "\n",
        "latin_a = Path(\"images_background/Latin/character01\")\n",
        "paths = list(latin_a.glob(\"*.png\"))\n",
        "\n",
        "_, axs = plt.subplots(3, 5, figsize=(15, 10))\n",
        "\n",
        "for i in range(15):\n",
        "    image = Image.open(paths[i])\n",
        "    axs[i % 3][i % 5].imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnzArRZjIpAk"
      },
      "source": [
        "We use `images_background` as train part of dataset and `images_evaluation` as valid part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLKviaYGBCkI"
      },
      "source": [
        "data = {\n",
        "    \"train\": {\"labels\": [], \"image_paths\": [], \"cls2label\": {}, \"root\": Path(\"images_background\")},\n",
        "    \"valid\": {\"labels\": [], \"image_paths\": [], \"cls2label\": {}, \"root\": Path(\"images_evaluation\")}\n",
        "}\n",
        "\n",
        "for part in data:\n",
        "    cur_part = data[part]\n",
        "    for alphabet_path in cur_part[\"root\"].iterdir():\n",
        "        for ch in alphabet_path.iterdir():\n",
        "            cur_part[\"labels\"].extend(f\"{alphabet_path.name}_{ch.name}\" for _ in ch.glob(\"*.png\"))\n",
        "            cur_part[\"image_paths\"].extend(ch.glob(\"*.png\"))\n",
        "    \n",
        "    cur_part[\"labels\"] = np.array(cur_part[\"labels\"])\n",
        "    cur_part[\"image_paths\"] = np.array(cur_part[\"image_paths\"])\n",
        "    cur_part[\"cls2label\"] = {c: num for num, c in enumerate(np.unique(cur_part[\"labels\"]))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RyJZHIgVO0p"
      },
      "source": [
        "## Metric Learning Pipeline\n",
        "\n",
        "Remember the Metric Learning pipeline in training stage\n",
        "\n",
        "![training stage](https://miro.medium.com/max/1400/1*_kwjkuV7MtJCCYriwwo3uA.png)\n",
        "\n",
        "and in validation stage \n",
        "\n",
        "![validating stage](https://miro.medium.com/max/1400/1*w3NVYqXA_e-EwWrvnvvrDw.png)\n",
        "\n",
        "\n",
        "(from [Metric Learning with Catalyst](https://medium.com/pytorch/metric-learning-with-catalyst-8c8337dfab1a) by [@AlexeySh](https://github.com/AlekseySh))\n",
        "\n",
        "To train model for Netric Learning objective, we need:\n",
        "\n",
        "- Metric Learning Dataset for training\n",
        "- Query/Gallery Dataset for validation\n",
        "- In-batch sampler of triplets\n",
        "- Triplet Loss criterion\n",
        "\n",
        "We have implemented this. But you need to understand some parts of this pipeline.\n",
        "That's why we left `HardTripletSampler` and `TripletMarginLossWithSampler`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hO8W9rWAOJe"
      },
      "source": [
        "### Data Loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbNLSRhki7HI"
      },
      "source": [
        "Metric Learning Dataset is similar with usual dataset. One difference is the `get_labels` method. It should return all labels in dataset. That used for class balanced batch sampler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjPyLMj0bucU"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from catalyst.utils import imread\n",
        "from catalyst.data import (\n",
        "    MetricLearningTrainDataset,\n",
        "    QueryGalleryDataset,\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class OmniGlotMLDataset(MetricLearningTrainDataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        images,\n",
        "        targets,\n",
        "        transform=None\n",
        "    ):\n",
        "        self.images = images\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.images[index], int(self.targets[index])\n",
        "        img = imread(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(image=img)[\"image\"].mean(0, keepdims=True)\n",
        "        \n",
        "        return img, target\n",
        "\n",
        "    # The most important method!\n",
        "    def get_labels(self):\n",
        "        return self.targets.tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.images.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9UymmY4jnSz"
      },
      "source": [
        "For validation, we need to create an Query-Gallery Dataset. Some of the object must be marked as query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi_-V_dpqWNw"
      },
      "source": [
        "class OmniGlotQGDataset(QueryGalleryDataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        images,\n",
        "        targets,\n",
        "        transform=None,\n",
        "        gallery_fraq=0.2,\n",
        "    ):\n",
        "        self._omniglot = OmniGlotMLDataset(\n",
        "            images=images, targets=targets, transform=transform\n",
        "        )\n",
        "\n",
        "        self._is_query = np.zeros(len(self._omniglot)).astype(bool)\n",
        "        labels = np.array(self._omniglot.get_labels())\n",
        "        self._query_size = 0\n",
        "        # creating query/gallery partition\n",
        "        for label in np.unique(targets):\n",
        "            query_size = \\\n",
        "                np.ceil(np.sum(labels == label) * gallery_fraq).astype(int)\n",
        "            # defining query size for current label\n",
        "            idx = np.arange(len(self._omniglot))[labels == label]\n",
        "            # indexes with current label\n",
        "            assert len(idx) >= query_size\n",
        "            \n",
        "            query_idx = np.random.choice(idx, size=query_size, replace=False)\n",
        "            # sampling query\n",
        "            self._is_query[query_idx] = True\n",
        "            # changing is_query flag\n",
        "            self._query_size += query_size\n",
        "            # incrementing query size\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        image, label = self._omniglot[idx]\n",
        "        return {\n",
        "            \"features\": image,\n",
        "            \"targets\": label,\n",
        "            \"is_query\": self._is_query[idx],\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._omniglot)\n",
        "\n",
        "    @property\n",
        "    def gallery_size(self):\n",
        "        return len(self._omniglot) - self._query_size\n",
        "\n",
        "    @property\n",
        "    def query_size(self):\n",
        "        return self._query_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfMjYZUtJkxc"
      },
      "source": [
        "We won't use augmentation in our pipeline. But the reason we'll use `albumentations` is that it has convinient tools for the image preprocessing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Hm6nievNak"
      },
      "source": [
        "import albumentations as albu\n",
        "from albumentations.pytorch import ToTensorV2 as ToTensor\n",
        "\n",
        "\n",
        "IMAGE_SIZE = 64\n",
        "\n",
        "transform = albu.Compose([\n",
        "    albu.LongestMaxSize(IMAGE_SIZE),\n",
        "    albu.PadIfNeeded(IMAGE_SIZE, IMAGE_SIZE, border_mode=0),\n",
        "    albu.Rotate(limit=30),\n",
        "    albu.Normalize(),\n",
        "    ToTensor()\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH1oDudDK1Pc"
      },
      "source": [
        "Process train/valid data, create datasets and loaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND43OdI9h7Q5"
      },
      "source": [
        "train_images = data[\"train\"][\"image_paths\"]\n",
        "train_labels = data[\"train\"][\"labels\"]\n",
        "\n",
        "indexes = np.arange(train_images.shape[0])\n",
        "np.random.shuffle(indexes)\n",
        "train_images = train_images[indexes]\n",
        "train_targets = np.array([data[\"train\"][\"cls2label\"][c] for c in train_labels])[indexes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTryxzI1Atkc"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from catalyst.data import BalanceBatchSampler\n",
        "\n",
        "dataset_train = OmniGlotMLDataset(images=train_images, targets=train_targets, transform=transform)\n",
        "sampler = BalanceBatchSampler(labels=dataset_train.get_labels(), p=64, k=16)\n",
        "train_loader = DataLoader(dataset=dataset_train, sampler=sampler, batch_size=sampler.batch_size)\n",
        "\n",
        "valid_images = data[\"valid\"][\"image_paths\"]\n",
        "valid_labels = data[\"valid\"][\"labels\"]\n",
        "valid_targets = np.array([data[\"valid\"][\"cls2label\"][c] for c in valid_labels])\n",
        "\n",
        "dataset_valid = OmniGlotQGDataset(images=valid_images, targets=valid_targets, transform=transform, gallery_fraq=0.2)\n",
        "valid_loader = DataLoader(dataset=dataset_valid, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8pjY2glAQnT"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfrefovqAjBk"
      },
      "source": [
        "from itertools import combinations, product\n",
        "from random import sample, choices\n",
        "from sys import maxsize\n",
        "\n",
        "from catalyst.data import InBatchTripletsSampler\n",
        "from catalyst.contrib.utils.misc import find_value_ids\n",
        "from catalyst.data.utils import convert_labels2list\n",
        "from catalyst.utils.torch import normalize\n",
        "\n",
        "\n",
        "class HardTripletsSampler(InBatchTripletsSampler):\n",
        "    \"\"\"\n",
        "    This sampler selects hardest triplets based on distances between features:\n",
        "    the hardest positive sample has the maximal distance to the anchor sample,\n",
        "    the hardest negative sample has the minimal distance to the anchor sample.\n",
        "    Note that a typical triplet loss chart is as follows:\n",
        "    1. Falling: loss decreases to a value equal to the margin.\n",
        "    2. Long plato: the loss oscillates near the margin.\n",
        "    3. Falling: loss decreases to zero.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, norm_required=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            norm_required: set True if features normalisation is needed\n",
        "        \"\"\"\n",
        "        self._norm_required = norm_required\n",
        "\n",
        "    def _sample(self, features, labels):\n",
        "        \"\"\"\n",
        "        This method samples the hardest triplets inside the batch.\n",
        "        Args:\n",
        "            features: has the shape of [batch_size, feature_size]\n",
        "            labels: labels of the samples in the batch\n",
        "        Returns:\n",
        "            the batch of the triplets in the order below:\n",
        "            (anchor, positive, negative)\n",
        "        \"\"\"\n",
        "        assert features.shape[0] == len(labels)\n",
        "\n",
        "        if self._norm_required:\n",
        "            features = normalize(samples=features.detach())\n",
        "\n",
        "        dist_mat = torch.cdist(x1=features, x2=features, p=2)\n",
        "\n",
        "        ids_anchor, ids_pos, ids_neg = self._sample_from_distmat(\n",
        "            distmat=dist_mat, labels=labels\n",
        "        )\n",
        "\n",
        "        return ids_anchor, ids_pos, ids_neg\n",
        "\n",
        "    @staticmethod\n",
        "    def _sample_from_distmat(distmat, labels):\n",
        "        \"\"\"\n",
        "        This method samples the hardest triplets based on the given\n",
        "        distances matrix. It chooses each sample in the batch as an\n",
        "        anchor and then finds the harderst positive and negative pair.\n",
        "        Args:\n",
        "            distmat: matrix of distances between the features\n",
        "            labels: labels of the samples in the batch\n",
        "        Returns:\n",
        "            the batch of triplets in the order below:\n",
        "            (anchor, positive, negative)\n",
        "        \"\"\"\n",
        "        ids_all = set(range(len(labels)))\n",
        "\n",
        "        ids_anchor, ids_pos, ids_neg = [], [], []\n",
        "\n",
        "        for i_anch, label in enumerate(labels):\n",
        "            ids_label = set(find_value_ids(it=labels, value=label))\n",
        "\n",
        "            ids_pos_cur = np.array(list(ids_label - {i_anch}), int)\n",
        "            ids_neg_cur = np.array(list(ids_all - ids_label), int)\n",
        "\n",
        "            i_pos = ids_pos_cur[distmat[i_anch, ids_pos_cur].argmax()]\n",
        "            i_neg = ids_neg_cur[distmat[i_anch, ids_neg_cur].argmin()]\n",
        "\n",
        "            ids_anchor.append(i_anch)\n",
        "            ids_pos.append(i_pos)\n",
        "            ids_neg.append(i_neg)\n",
        "\n",
        "        return ids_anchor, ids_pos, ids_neg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmlFkb9HLndy"
      },
      "source": [
        "Torch has the TripletLoss criterion. But it doesn't work with in-batch sampler. We must join these two enteties in new one. It has to sample triplets by the given rule and calculate triplet loss with the margin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q0UsHNtAkJs"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class TripletMarginLossWithSampler(nn.Module):\n",
        "    \"\"\"\n",
        "    This class combines in-batch sampling of triplets and\n",
        "    default TripletMargingLoss from PyTorch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin, sampler_inbatch):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            margin: margin value\n",
        "            sampler_inbatch: sampler for forming triplets inside the batch\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self._sampler_inbatch = sampler_inbatch\n",
        "        self._triplet_margin_loss = nn.TripletMarginLoss(margin=margin)\n",
        "\n",
        "    def forward(self, features, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features: features with the shape of [batch_size, features_dim]\n",
        "            labels: labels of samples having batch_size elements\n",
        "        Returns: loss value\n",
        "        \"\"\"\n",
        "        labels_list = convert_labels2list(labels)\n",
        "\n",
        "        (\n",
        "            features_anchor,\n",
        "            features_positive,\n",
        "            features_negative,\n",
        "        ) = self._sampler_inbatch.sample(features=features, labels=labels_list)\n",
        "\n",
        "        loss = self._triplet_margin_loss(\n",
        "            anchor=features_anchor,\n",
        "            positive=features_positive,\n",
        "            negative=features_negative,\n",
        "        )\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbHDxzRvMWNr"
      },
      "source": [
        "Create simple model with pretrained MobileNet backbone. It's able to increase training time. Good model must be trained for long time (several hundreads epochs). For educational purposes, we adjust train time to one epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULcfnZSWVUJI"
      },
      "source": [
        "from catalyst import dl, utils\n",
        "from catalyst.contrib.nn import TripletMarginLossWithSampler, RAdam\n",
        "from torchvision.models import mobilenet_v2\n",
        "from catalyst.contrib.nn import Normalize\n",
        "\n",
        "\n",
        "num_epochs = 300\n",
        "lr = 1e-2\n",
        "\n",
        "def conv_block(in_feature, out_feature):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_feature, out_feature, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(out_feature),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2, stride=2)\n",
        "    )\n",
        "\n",
        "model = nn.Sequential(\n",
        "    conv_block(1, 4),\n",
        "    conv_block(4, 8),\n",
        "    conv_block(8, 16),\n",
        "    conv_block(16, 32),\n",
        "    conv_block(32, 64),\n",
        "    nn.Flatten(),\n",
        "    Normalize()\n",
        ")\n",
        "optimizer = RAdam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [100, 200], gamma = 0.1)\n",
        "\n",
        "sampler_inbatch = HardTripletsSampler()\n",
        "criterion = TripletMarginLossWithSampler(margin=0.3, sampler_inbatch=sampler_inbatch)\n",
        "\n",
        "callbacks = [\n",
        "    dl.ControlFlowCallback(\n",
        "        dl.CriterionCallback(), \n",
        "        loaders=\"train\"\n",
        "    ),\n",
        "    dl.ControlFlowCallback(dl.CMCScoreCallback(topk_args=[1, 5, 10]), loaders=\"valid\"),\n",
        "    dl.PeriodicLoaderCallback(valid=10),\n",
        "    dl.SchedulerCallback()\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxUcNn0MB0HS",
        "scrolled": true
      },
      "source": [
        "runner = dl.SupervisedRunner(device=utils.get_device())\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    callbacks=callbacks,\n",
        "    loaders={\"train\": train_loader, \"valid\": valid_loader},\n",
        "    minimize_metric=False,\n",
        "    verbose=True,\n",
        "    valid_loader=\"valid\",\n",
        "    num_epochs=num_epochs,\n",
        "    main_metric=\"cmc05\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m3zqnG_dLtP"
      },
      "source": [
        "# Now try to implement sampler and loss by yourself!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqhhrZWgdLtP"
      },
      "source": [
        "## Hard Cluster Triplet Sampler\n",
        "\n",
        "**tl;dr**\n",
        "\n",
        "The target is to minimise intra class variations and to maximize the inter class variations.\n",
        "\n",
        "First of all we counting \"centroids\" of a class:\n",
        "$$\n",
        "f_{i}^{m}=\\frac{\\sum^{K} f(x)}{K}\n",
        "$$\n",
        "\n",
        "then we are counting intra class loss:\n",
        "$$\n",
        "d_{i}^{i n t r a}=\\max _{K}\\left\\|f(x)-f_{i}^{m}\\right\\|_{2}^{2}\n",
        "$$\n",
        "and inter class loss:\n",
        "$$\n",
        "d_{i}^{\\text {inter}}=\\min _{\\forall i_{d} \\in P, i_{d} \\neq i}\\left\\|f_{i}^{m}-f_{i_{d}}^{m}\\right\\|_{2}^{2}\n",
        "$$\n",
        "\n",
        "The final loss is:\n",
        "$$\n",
        "L b_{c}=\\sum_{i}^{P} \\max \\left(\\left(d_{i}^{i n t r a}-d_{i}^{i n t e r}+\\alpha\\right), 0\\right)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "[article](https://arxiv.org/abs/1812.10325)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpoNiq2ZdLtP"
      },
      "source": [
        "from catalyst.data import IInbatchTripletSampler\n",
        "from collections import Counter\n",
        "\n",
        "class HardClusterTripletsSampler(IInbatchTripletSampler):\n",
        "\n",
        "    def _check_input_labels(self, labels):\n",
        "        labels_counter = Counter(labels)\n",
        "        assert all(n > 1 for n in labels_counter.values())\n",
        "        assert len(labels_counter) > 1\n",
        "\n",
        "    def sample(self, features, labels):\n",
        "        self._check_input_labels(labels=labels)\n",
        "        clusters_features, mapping = self.sample_cluster_features(features, labels)\n",
        "        anchor_features, pos_features, neg_features = [], [], []\n",
        "\n",
        "        for i_anch, label in enumerate(labels):\n",
        "            ids_label = np.array(find_value_ids(it=labels, value=label))\n",
        "            dist_pos = torch.cdist(x1=clusters_features[mapping[label]].unsqueeze(0), x2=features[ids_label], p=2)\n",
        "            dist_neg = torch.cdist(x1=clusters_features[mapping[label]].unsqueeze(0), x2=clusters_features, p=2)\n",
        "            dist_neg = dist_neg.topk(2, largest = False)[1]\n",
        "            dist_neg = dist_neg[dist_neg != mapping[label]]\n",
        "            dist_neg = dist_neg[:min(len(dist_neg), 1)]\n",
        "\n",
        "            neg_feature = clusters_features[dist_neg].squeeze()\n",
        "            pos_feature = features[ids_label[dist_pos[0].argmax()]]\n",
        "\n",
        "            anchor_features.append(features[i_anch])\n",
        "            pos_features.append(pos_feature)\n",
        "            neg_features.append(neg_feature)\n",
        "        \n",
        "        anchors = torch.stack(anchor_features)\n",
        "        negatives = torch.stack(neg_features)\n",
        "        positives = torch.stack(pos_features)\n",
        "        \n",
        "        return anchors, positives, negatives\n",
        "        \n",
        "    def sample_cluster_features(self, features, labels):\n",
        "        set_labels = np.unique(labels)\n",
        "        cluster_features = []\n",
        "        mapping = dict(zip(set_labels, range(len(set_labels))))\n",
        "\n",
        "        for label in set_labels:\n",
        "          ids_label = np.array(find_value_ids(it=labels, value=label))\n",
        "          cluster = features[ids_label].mean(dim = 0)\n",
        "          cluster_features.append(cluster)\n",
        "\n",
        "        cluster_features = torch.stack(cluster_features)\n",
        "        return cluster_features, mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E7HPa5wdLtR"
      },
      "source": [
        "num_epochs = 200\n",
        "lr = 1e-2\n",
        "\n",
        "model = nn.Sequential(\n",
        "    conv_block(1, 4),\n",
        "    conv_block(4, 8),\n",
        "    conv_block(8, 16),\n",
        "    conv_block(16, 32),\n",
        "    conv_block(32, 64),\n",
        "    nn.Flatten(),\n",
        "    Normalize()\n",
        ")\n",
        "optimizer = RAdam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [100], gamma = 0.1)\n",
        "\n",
        "sampler_inbatch = HardClusterTripletsSampler()\n",
        "criterion = TripletMarginLossWithSampler(margin=0.3, sampler_inbatch=sampler_inbatch)\n",
        "\n",
        "callbacks = [\n",
        "    dl.ControlFlowCallback(\n",
        "        dl.CriterionCallback(), \n",
        "        loaders=\"train\"\n",
        "    ),\n",
        "    dl.ControlFlowCallback(dl.CMCScoreCallback(topk_args=[1, 5, 10]), loaders=\"valid\"),\n",
        "    dl.PeriodicLoaderCallback(valid=10),\n",
        "    dl.SchedulerCallback(),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AGLdMagdLtT"
      },
      "source": [
        "runner = dl.SupervisedRunner(device=utils.get_device())\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    callbacks=callbacks,\n",
        "    loaders={\"train\": train_loader, \"valid\": valid_loader},\n",
        "    minimize_metric=False,\n",
        "    verbose=True,\n",
        "    valid_loader=\"valid\",\n",
        "    num_epochs=num_epochs,\n",
        "    main_metric=\"cmc05\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMHlEFjKKwoA"
      },
      "source": [
        "from tqdm import tqdm\n",
        "tqdm._instances.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5W-_SRtdLtV"
      },
      "source": [
        "## Hierarchical Triplet Loss\n",
        "\n",
        "![](https://cdn.mathpix.com/snip/images/1TyIaVSJSOgWcwLS62bmsMFJTG5rDGKYjUVqRDdZi3w.original.fullsize.png)\n",
        "\n",
        "Here we will construct hierarchical structure of classes based on the computed interclass distances.\n",
        "\n",
        "So the algoruthm looks like:\n",
        "\n",
        "![](https://cdn.mathpix.com/snip/images/pYM4D0nkTjPxRpMWzOth3Duxnyj8sKdLpU4xXmeu7SQ.original.fullsize.png)\n",
        "\n",
        "You can try to build a tree in a way that every leaf represents a letter and every root of this leaf represents an alphabet.\n",
        "In this case, your tree has a depth of 3. Also, you can skip the step with updating the tree.\n",
        "\n",
        "\n",
        "[article](https://arxiv.org/pdf/1810.06951.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB07dcJCv-Ev"
      },
      "source": [
        "from catalyst.data import IInbatchTripletSampler\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# по сути нам нужен этот класс чтобы считать dynamic margin\n",
        "class HierarchicalTripletMargin:\n",
        "\n",
        "    def __init__(self, model, loader, betta = 0.2):\n",
        "      self.betta = betta\n",
        "      self.model = model.to(utils.get_device())\n",
        "      self.loader = loader\n",
        "      features, labels = self.get_features()\n",
        "      self.distances = self.average_distances(features, labels) #all features and labels\n",
        "    \n",
        "    def get_margin(self, labels):\n",
        "      d_0 = self.distances.mean()\n",
        "      d_1 = (4-d_0)/2+d_0\n",
        "      s_y = self.distances[labels]\n",
        "      margins = self.betta + d_1 - s_y\n",
        "      margins = self.betta + d_1\n",
        "      return margins\n",
        "\n",
        "    def average_distances(self, features, labels):\n",
        "      set_labels = np.unique(labels)\n",
        "      set_labels.sort()\n",
        "      distances = [] #in sorted order: 0 label, 1 label and etc...\n",
        "      for label in tqdm(set_labels):\n",
        "          ids_label = np.array(find_value_ids(it=labels, value=label))\n",
        "          matrix = torch.cdist(x1=features[ids_label], x2=features[ids_label], p=2)\n",
        "          avg_dist = matrix.sum()/((matrix.shape[0]-1)*matrix.shape[0])\n",
        "          distances.append(avg_dist)\n",
        "      \n",
        "      distances = torch.stack(distances)\n",
        "      return distances\n",
        "\n",
        "    def get_features(self):\n",
        "      self.model.eval()\n",
        "      features = []\n",
        "      labels = []\n",
        "      with torch.no_grad():\n",
        "        for X, y in self.loader:\n",
        "          features.append(self.model(X.to(utils.get_device())))\n",
        "          labels.append(y)\n",
        "\n",
        "      features = torch.cat(features)\n",
        "      labels = torch.cat(labels)\n",
        "      return features, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdFUa0pL7Zv6"
      },
      "source": [
        "class HierarchicalTripletLoss(nn.Module):\n",
        "\n",
        "  def forward(self, anchors, positives, negatives, margins):\n",
        "    anch_pos = F.pairwise_distance(anchors, positives)\n",
        "    anch_neg = F.pairwise_distance(anchors, negatives)\n",
        "    loss = anch_pos - anch_neg + margins\n",
        "    triplet_loss = F.relu(loss).mean()\n",
        "    return triplet_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGuo9jF8dLtW"
      },
      "source": [
        "from torch.nn.functional import triplet_margin_loss\n",
        "\n",
        "\n",
        "class HierarchicalTripletMarginLossWithSampler(nn.Module):\n",
        "    \"\"\"\n",
        "    This class combines in-batch sampling of triplets and\n",
        "    default TripletMargingLoss from PyTorch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sampler_inbatch, model, loader):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sampler_inbatch: sampler for forming triplets inside the batch\n",
        "            model: model\n",
        "            loader: loader\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self._sampler_inbatch = sampler_inbatch\n",
        "        self._triplet_margin_loss = HierarchicalTripletLoss()\n",
        "        self._margins = HierarchicalTripletMargin(model, loader)\n",
        "\n",
        "    def forward(self, features, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features: features with the shape of [batch_size, features_dim]\n",
        "            labels: labels of samples having batch_size elements\n",
        "        Returns: loss value\n",
        "        \"\"\"\n",
        "        labels_list = convert_labels2list(labels)\n",
        "\n",
        "        (\n",
        "            features_anchor,\n",
        "            features_positive,\n",
        "            features_negative,\n",
        "        ) = self._sampler_inbatch.sample(features=features, labels=labels_list)\n",
        "        \n",
        "        margins = self._margins.get_margin(labels) # Calculate margin based on label\n",
        "\n",
        "        loss = self._triplet_margin_loss(\n",
        "            anchors=features_anchor,\n",
        "            positives=features_positive,\n",
        "            negatives=features_negative,\n",
        "            margins=margins\n",
        "        )\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUYevLjydLtY"
      },
      "source": [
        "num_epochs = 300\n",
        "lr = 1e-1\n",
        "\n",
        "model = nn.Sequential(\n",
        "    conv_block(1, 4),\n",
        "    conv_block(4, 8),\n",
        "    conv_block(8, 16),\n",
        "    conv_block(16, 32),\n",
        "    conv_block(32, 64),\n",
        "    nn.Flatten(),\n",
        "    Normalize()\n",
        ")\n",
        "optimizer = RAdam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [100,200])\n",
        "\n",
        "sampler_inbatch = HardTripletsSampler()\n",
        "criterion = HierarchicalTripletMarginLossWithSampler(sampler_inbatch, model, train_loader)\n",
        "\n",
        "callbacks = [\n",
        "    dl.ControlFlowCallback(\n",
        "        dl.CriterionCallback(), \n",
        "        loaders=\"train\"\n",
        "    ),\n",
        "    dl.ControlFlowCallback(dl.CMCScoreCallback(topk_args=[1, 5, 10]), loaders=\"valid\"),\n",
        "    dl.PeriodicLoaderCallback(valid=10),\n",
        "    dl.SchedulerCallback(),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kly6hZQCdLta"
      },
      "source": [
        "runner = dl.SupervisedRunner(device=utils.get_device())\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    callbacks=callbacks,\n",
        "    loaders={\"train\": train_loader, \"valid\": valid_loader},\n",
        "    minimize_metric=False,\n",
        "    verbose=True,\n",
        "    valid_loader=\"valid\",\n",
        "    num_epochs=num_epochs,\n",
        "    main_metric=\"cmc05\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCXbpo6RF0HM"
      },
      "source": [
        "from tqdm import tqdm\n",
        "tqdm._instances.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buFkfSNXdLtd"
      },
      "source": [
        "### Optional Task\n",
        "If you implemented all methods quickly and sure about correctness, you can try to compare three methods.\n",
        "But it takes time and resources to compare the methods properly.\n",
        "\n",
        "**results:**\n",
        "\n",
        "HardTriplet cmc01=0.7018 | cmc05=0.9074 | cmc10=0.9514\n",
        "\n",
        "HardClusterTriplet cmc01=0.5903 | cmc05=0.8342 | cmc10=0.9017\n",
        "\n",
        "HardHierarchical cmc01=0.7394 | cmc05=0.9177 | cmc10=0.9545"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7fSKHy54qth"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}